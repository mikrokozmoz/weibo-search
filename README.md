## 本项目说明 | Project Statement

本项目是基于 [dataabc/weibo-search](https://github.com/dataabc/weibo-search) 的 fork，并在原有基础上做了若干改动。原项目版权归原作者所有，遵循其开源协议（MIT License）。如有侵权请联系删除。

*This project is a fork based on [dataabc/weibo-search](https://github.com/dataabc/weibo-search), with several modifications on top of the original. Copyright belongs to the original author and follows the original open source license (MIT License). Please contact for removal if there is any infringement.*

## 本地主要改动 | Main Local Changes

- 支持通过 `HOT_SORT` 参数控制是否按热度排序（settings.py 和 search.py 增加了相关逻辑）。
- 支持 `START_DATE` 和 `END_DATE` 可为空或 None，不限制日期时可全量抓取。
- 优化了翻页和细分逻辑，保证热度排序参数在所有请求中都能正确传递。
- 其他细节优化和兼容性调整。

<br>

- *Support for controlling hot sort via `HOT_SORT` parameter (logic added in settings.py and search.py).*
- *Allow `START_DATE` and `END_DATE` to be None or empty, enabling unrestricted date crawling.*
- *Improved pagination and subdivision logic to ensure hot sort parameters are correctly passed in all requests.*(`LIMIT_RESULT`).*
- *Other minor optimizations and compatibility adjustments.*

## 功能 | Features

连续获取一个或多个**微博关键词搜索**结果，并将结果写入文件（可选）、数据库（可选）等。所谓微博关键词搜索即：**搜索正文中包含指定关键词的微博**，可以指定搜索的时间范围。
<br>
>举个栗子，比如你可以搜索包含关键词“迪丽热巴”且发布日期在2020-03-01和2020-03-16之间的微博。搜索结果数量巨大，对于非常热门的关键词，在一天的指定时间范围，可以获得**1000万**以上的搜索结果。注意这里的一天指的是时间筛选范围，具体多长时间将这1000万微博下载到本地还要看获取的速度。1000万只是一天时间范围可获取的微博数量，如果想获取更多微博，可以加大时间范围，比如10天，最多可以获得1000万X10=1亿条搜索结果，当然你也可以再加大时间范围。对于大多数关键词，微博一天产生的相关搜索结果应该低于1000万，因此可以说**本程序可以获取指定关键词的全部或近似全部的搜索结果**。本程序可以获得几乎全部的微博信息，如微博正文、发布者等，详情见[输出](#输出)部分。

支持输出多种文件类型，具体如下：
- 写入**csv文件**（默认）
- 写入**MySQL数据库**（可选）
- 写入**MongoDB数据库**（可选）
- 写入**Sqlite数据库**（可选，无需外部安装，相比MySQL和MongoDB更方便）
- 下载微博中的**图片**（可选）
- 下载微博中的**视频**（可选）

<br>

*Continuously fetch one or more **Weibo keyword search** results and write them to files (optional), databases (optional), etc. "Weibo keyword search" means: **search for Weibo posts containing the specified keyword in the content**, and you can specify the time range for the search.*<br>
>*For example, you can search for Weibo posts containing the keyword "Dilireba" and published between 2020-03-01 and 2020-03-16. The number of search results is huge. For very popular keywords, you can get more than **10 million** search results in a specified one-day time range. Note that one day here refers to the time filter range, and how long it takes to download these 10 million Weibo posts locally depends on the speed. 10 million is just the number of Weibo posts that can be obtained in one day. If you want more, you can increase the time range, such as 10 days, up to 100 million results, and you can increase it further. For most keywords, the number of related search results generated by Weibo in one day should be less than 10 million, so **this program can obtain all or nearly all search results for the specified keyword**. The program can obtain almost all Weibo information, such as content, publisher, etc. See the [Output](#输出) section for details.*

*Supports multiple output file types, as follows:*
- *Write to **csv file** (default)*
- *Write to **MySQL database** (optional)*
- *Write to **MongoDB database** (optional)*
- *Write to **Sqlite database** (optional, no external installation required, more convenient than MySQL and MongoDB)*
- *Download **images** in Weibo (optional)*
- *Download **videos** in Weibo (optional)*

## 输出 | Output

- 微博id：微博的id，为一串数字形式
- 微博bid：微博的bid
- 微博内容：微博正文
- 头条文章url：微博中头条文章的url，若某微博中不存在头条文章，则该值为''
- 原始图片url：原创微博图片和转发微博转发理由中图片的url，若某条微博存在多张图片，则每个url以英文逗号分隔，若没有图片则值为''
- 视频url: 微博中的视频url和Live Photo中的视频url，若某条微博存在多个视频，则每个url以英文分号分隔，若没有视频则值为''
- 微博发布位置：位置微博中的发布位置
- 微博发布时间：微博发布时的时间，精确到天
- 点赞数：微博被赞的数量
- 转发数：微博被转发的数量
- 评论数：微博被评论的数量
- 微博发布工具：微博的发布工具，如iPhone客户端、HUAWEI Mate 20 Pro等，若没有则值为''
- 话题：微博话题，即两个#中的内容，若存在多个话题，每个url以英文逗号分隔，若没有则值为''
- @用户：微博@的用户，若存在多个@用户，每个url以英文逗号分隔，若没有则值为''
- 原始微博id：为转发微博所特有，是转发微博中那条被转发微博的id，那条被转发的微博也会存储，字段和原创微博一样，只是它的本字段为空
- 结果文件：保存在当前目录“结果文件”文件夹下以关键词为名的文件夹里
- 微博图片：微博中的图片，保存在以关键词为名的文件夹下的images文件夹里
- 微博视频：微博中的视频，保存在以关键词为名的文件夹下的videos文件夹里
- user_authentication：微博用户类型，值分别是`蓝v`，`黄v`，`红v`，`金v`和`普通用户`

<br>

- *Weibo id: The id of the Weibo post, a string of numbers*
- *Weibo bid: The bid of the Weibo post*
- *Weibo content: The text of the Weibo post*
- *Headline article url: The url of the headline article in the Weibo post, if not present, value is ''*
- *Original image url: The url of images in original or retweeted Weibo, separated by commas if multiple, '' if none*
- *Video url: The url of videos in Weibo and Live Photo, separated by semicolons if multiple, '' if none*
- *Weibo post location: The location where the Weibo was posted*
- *Weibo post time: The time when the Weibo was posted, accurate to the day*
- *Likes: Number of likes*
- *Reposts: Number of reposts*
- *Comments: Number of comments*
- *Weibo post tool: The tool used to post, e.g., iPhone client, HUAWEI Mate 20 Pro, '' if none*
- *Topics: Weibo topics, content between two #, separated by commas if multiple, '' if none*
- *@Users: Users mentioned in the Weibo, separated by commas if multiple, '' if none*
- *Original Weibo id: For retweets, the id of the original Weibo being retweeted, also stored as a normal Weibo, this field is empty for original posts*
- *Result files: Saved in a folder named after the keyword under the "结果文件" directory*
- *Weibo images: Images in Weibo, saved in the images folder under the keyword folder*
- *Weibo videos: Videos in Weibo, saved in the videos folder under the keyword folder*
- *user_authentication: Weibo user type, values are `蓝v`, `黄v`, `红v`, `金v`, and `普通用户`*

## 使用说明 | Usage

本程序的所有配置都在setting.py文件中完成，该文件位于“weibo-search\weibo\settings.py”。

*All configuration is done in the setting.py file, located at "weibo-search\weibo\settings.py".*

### 1.下载脚本 | Download script

```bash
$ git clone https://github.com/mikrokozmoz/weibo-search.git
```

### 2.安装Scrapy | Install Scrapy

本程序依赖Scrapy，要想运行程序，需要安装Scrapy。如果系统中没有安装Scrapy，请根据自己的系统安装Scrapy，以Ubuntu为例，可以使用如下命令：

*This program depends on Scrapy. To run the program, you need to install Scrapy. If Scrapy is not installed, please install it according to your system. For Ubuntu, you can use the following command:*

```bash
$ pip install scrapy
```

### 3.安装依赖 | Install dependencies

```bash
$ pip install -r requirements.txt
```

### 4.设置cookie | Set cookie

DEFAULT_REQUEST_HEADERS中的cookie是我们需要填的值，如何获取cookie详见[如何获取cookie](#如何获取cookie)，获取后将"your cookie"替换成真实的cookie即可。

*The cookie in DEFAULT_REQUEST_HEADERS is the value you need to fill in. See [How to get cookie](#如何获取cookie) for details. After obtaining it, replace "your cookie" with the real cookie.*

### 5.设置搜索关键词 | Set search keywords

修改setting.py文件夹中的KEYWORD_LIST参数。
如果你想搜索一个关键词，如“迪丽热巴”：

*Modify the KEYWORD_LIST parameter in the setting.py file.*
*If you want to search for a single keyword, e.g., "迪丽热巴":*

```
KEYWORD_LIST = ['迪丽热巴']
```

如果你想分别搜索多个关键词，如想要分别获得“迪丽热巴”和“杨幂”的搜索结果：

*If you want to search for multiple keywords separately, e.g., "迪丽热巴" and "杨幂":*

```
KEYWORD_LIST = ['迪丽热巴', '杨幂']
```

如果你想搜索同时包含多个关键词的微博，如同时包含“迪丽热巴”和“杨幂”微博的搜索结果：

*If you want to search for Weibo posts containing multiple keywords at the same time, e.g. "迪丽热巴" and "杨幂":*

```
KEYWORD_LIST = ['迪丽热巴 杨幂']
```

如果你想搜索微博话题，即包含#的内容，如“#迪丽热巴#”：

*If you want to search for Weibo topics, i.e., content with #, e.g., "#迪丽热巴#":*

```
KEYWORD_LIST = ['#迪丽热巴#']
```

也可以把关键词写进txt文件里，然后将txt文件路径赋值给KEYWORD_LIST，如：

*You can also write keywords into a txt file and assign the file path to KEYWORD_LIST, e.g.:*

```
KEYWORD_LIST = 'keyword_list.txt'
```

txt文件中每个关键词占一行。

*Each keyword occupies one line in the txt file.*

### 6.设置搜索时间范围 | Set search time range

START_DATE代表搜索的起始日期，END_DATE代表搜索的结束日期，值为“yyyy-mm-dd”形式，程序会搜索包含关键词且发布时间在起始日期和结束日期之间的微博（包含边界）。比如我想筛选发布时间在2020-06-01到2020-06-02这两天的微博：

*START_DATE is the start date, END_DATE is the end date, both in "yyyy-mm-dd" format. The program will search for Weibo posts containing the keyword and published between the start and end dates (inclusive). For example, to filter posts published between 2020-06-01 and 2020-06-02:*

```
START_DATE = '2020-06-01'
END_DATE = '2020-06-02'
```

### 7.设置FURTHER_THRESHOLD（可选）| Set FURTHER_THRESHOLD (optional)

FURTHER_THRESHOLD是程序是否进一步搜索的阈值。一般情况下，如果在某个搜索条件下，搜索结果很多，则搜索结果应该有50页微博，多于50页不显示。当总页数等于50时，程序认为搜索结果可能没有显示完全，所以会继续细分。比如，若当前是按天搜索的，程序会把当前的1个搜索分成24个搜索，每个搜索条件粒度是小时。这样就能获取在天粒度下无法获取完全的微博。同理，如果小时粒度下总页数仍然是50，会继续细分，以此类推。然而，有一些关键词，搜索结果即便很多，也只显示40多页。所以此时如果FURTHER_THRESHOLD是50，程序会认为只有这么多微博，不再继续细分，导致很多微博没有获取。因此为了获取更多微博，FURTHER_THRESHOLD应该是小于50的数字。但是如果设置的特别小，如1，这样即便结果真的只有几页，程序也会细分，这些没有必要的细分会使程序速度降低。因此，建议**FURTHER_THRESHOLD的值设置在40与46之间**：

*FURTHER_THRESHOLD is the threshold for further subdivision. Generally, if there are many search results, there should be 50 pages of Weibo. If the total number of pages is 50, the program thinks the results may not be fully displayed, so it will subdivide further. For example, if searching by day, the program will split the search into 24 searches by hour. This way, more Weibo posts can be obtained. If the number of pages is still 50 at the hour level, it will continue to subdivide. However, for some keywords, even if there are many results, only about 40 pages are shown. If FURTHER_THRESHOLD is 50, the program will think there are only so many posts and will not subdivide, resulting in missing posts. To get more, FURTHER_THRESHOLD should be less than 50. But if set too small, e.g., 1, unnecessary subdivision will slow down the program. It is recommended to set **FURTHER_THRESHOLD between 40 and 46**:*

```
FURTHER_THRESHOLD = 46
```

### 8.设置结果保存类型（可选）| Set result save type (optional)

ITEM_PIPELINES是我们可选的结果保存类型，第一个代表去重，第二个代表写入csv文件，第三个代表写入MySQL数据库，第四个代表写入MongDB数据库，第五个代表下载图片，第六个代表下载视频。后面的数字代表执行的顺序，数字越小优先级越高。如果你只要写入部分类型，可以把不需要的类型用“#”注释掉，以节省资源；如果你想写入数据库，需要在setting.py填写相关数据库的配置。

*ITEM_PIPELINES are the optional result save types. The first is deduplication, the second is writing to csv, the third is writing to MySQL, the fourth is writing to MongoDB, the fifth is downloading images, and the sixth is downloading videos. The number indicates the execution order, the smaller the higher the priority. If you only want some types, comment out the others with "#" to save resources. If you want to write to a database, fill in the relevant configuration in setting.py.*

### 9.设置等待时间（可选）| Set wait time (optional)

DOWNLOAD_DELAY代表访问完一个页面再访问下一个时需要等待的时间，默认为10秒。如我想设置等待15秒左右，可以修改setting.py文件的DOWNLOAD_DELAY参数：

*DOWNLOAD_DELAY is the wait time after visiting a page before visiting the next, default is 10 seconds. To set it to about 15 seconds, modify the DOWNLOAD_DELAY parameter in setting.py:*

```
DOWNLOAD_DELAY = 15
```

### 10.设置微博类型（可选）| Set Weibo type (optional)

WEIBO_TYPE筛选要搜索的微博类型，0代表搜索全部微博，1代表搜索全部原创微博，2代表热门微博，3代表关注人微博，4代表认证用户微博，5代表媒体微博，6代表观点微博。比如我想要搜索全部原创微博，修改setting.py文件的WEIBO_TYPE参数：

*WEIBO_TYPE filters the type of Weibo to search. 0 for all, 1 for original, 2 for hot, 3 for followed users, 4 for verified users, 5 for media, 6 for viewpoints. For example, to search for all original Weibo, modify WEIBO_TYPE in setting.py:*

```
WEIBO_TYPE = 1
```

### 11.设置包含内容（可选）| Set content to include (optional)

CONTAIN_TYPE筛选结果微博中必需包含的内容，0代表不筛选，获取全部微博，1代表搜索包含图片的微博，2代表包含视频的微博，3代表包含音乐的微博，4代表包含短链接的微博。比如我想筛选包含图片的微博，修改setting.py文件的CONTAIN_TYPE参数：

*CONTAIN_TYPE filters the content that must be included in the results. 0 for all, 1 for images, 2 for videos, 3 for music, 4 for short links. For example, to filter for Weibo with images, modify CONTAIN_TYPE in setting.py:*

```
CONTAIN_TYPE = 1
```

### 12.筛选微博发布地区（可选）| Filter by region (optional)

REGION筛选微博的发布地区，精确到省或直辖市，值不应包含“省”或“市”等字，如想筛选北京市的微博请用“北京”而不是“北京市”，想要筛选安徽省的微博请用“安徽”而不是“安徽省”，可以写多个地区，具体支持的地名见region.py文件，注意只支持省或直辖市的名字，省下面的市名及直辖市下面的区县名不支持，不筛选请用”全部“。比如我想要筛选发布地在山东省的微博：

*REGION filters the region where the Weibo was posted, accurate to province or municipality. Do not include "省" or "市" in the value. For example, use "北京" instead of "北京市", "安徽" instead of "安徽省". Multiple regions can be specified. Supported names are in region.py. Only province or municipality names are supported, not city or district names. To not filter, use "全部". For example, to filter for posts from Shandong:*

```
REGION = ['山东']
```

### 13.配置数据库（可选）| Configure database (optional)

MONGO_URI是MongoDB数据库的配置；<br>
MYSQL开头的是MySQL数据库的配置。

<br>

MONGO_URI is the MongoDB configuration;<br>
MYSQL_* are MySQL configurations.

### 14.运行程序 | Run the program

```bash
$ scrapy crawl search -s JOBDIR=crawls/search
```

其实只运行“scrapy crawl search”也可以，只是上述方式在结束时可以保存进度，下次运行时会在程序上次的地方继续获取。注意，如果想要保存进度，请使用“Ctrl + C”**一次**，注意是**一次**。按下“Ctrl + C”一次后，程序会继续运行一会，主要用来保存获取的数据、保存进度等操作，请耐心等待。下次再运行时，只要再运行上面的指令就可以恢复上次的进度。如果再次运行没有结果，可能是进度没有正确保存，可以先删除crawls文件夹内的进度文件，再运行上述命令。

*You can also just run "scrapy crawl search". The above way saves progress, so you can continue from where you left off next time. Note: to save progress, use "Ctrl + C" **once**. After pressing "Ctrl + C" once, the program will continue running for a while to save data and progress. Please wait patiently. Next time, just run the above command to resume. If there are no results, progress may not have been saved correctly. You can delete the progress files in the crawls folder and run the command again.*

## 如何获取cookie | How to get cookie

1. 用Chrome打开 https://weibo.com/
2. 点击"立即登录", 完成私信验证或手机验证码验证, 进入新版微博. 如下图所示:

<img src="https://user-images.githubusercontent.com/41314224/144813569-cfb5ad32-22f0-4841-afa9-83184b2ccf6f.png" width="400px" alt="...">

3. 按F12打开开发者工具, 在开发者工具的 Network->Name->weibo.cn->Headers->Request Headers, 找到"Cookie:"后的值, 这就是我们要找的cookie值, 复制即可, 如图所示:

<img src="https://github.com/dataabc/media/blob/master/weiboSpider/images/cookie3.png" width="400px" alt="...">

<br>

1. *Open https://weibo.com/ with Chrome*
2. *Click "Login", complete verification, and enter the new Weibo. See below:*

<img src="https://user-images.githubusercontent.com/41314224/144813569-cfb5ad32-22f0-4841-afa9-83184b2ccf6f.png" width="400px" alt="...">

3. *Press F12 to open Developer Tools, go to Network->Name->weibo.cn->Headers->Request Headers, find the value after "Cookie:", copy it, as shown:*

<img src="https://github.com/dataabc/media/blob/master/weiboSpider/images/cookie3.png" width="400px" alt="...">

> ## 兼容性说明: 获取旧版微博的Cookie | Compatibility: Get old Weibo cookie

> 1. 用Chrome打开<https://passport.weibo.cn/signin/login>；
> 2. 输入微博的用户名、密码，登录，如图所示：
> 登录成功后会跳转到<https://m.weibo.cn>;
> 3. 按F12键打开Chrome开发者工具，在地址栏输入并跳转到<https://weibo.cn>，跳转后会显示如下类似界面:
> 4. 依此点击Chrome开发者工具中的Network->Name中的weibo.cn->Headers->Request Headers，"Cookie:"后的值即为我们要找的cookie值，复制即可，如图所示：

<br>

> 1. Open <https://passport.weibo.cn/signin/login> in Chrome;<br>
> 2. Enter username and password, login, as shown:
> After login, go to <https://m.weibo.cn>;<br>
> 3. Press F12 to open Developer Tools, go to <https://weibo.cn>, you will see a similar page:
> 4. In Developer Tools, go to Network->Name->weibo.cn->Headers->Request Headers, copy the value after "Cookie:", as shown:
